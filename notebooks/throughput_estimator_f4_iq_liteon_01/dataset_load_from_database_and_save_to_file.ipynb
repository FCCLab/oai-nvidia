{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "511d428d-e058-4e14-8c8b-f6e0b78bc279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.10/dist-packages (from torch) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch) (8.5.0.96)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.10/dist-packages (from torch) (2.14.3)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.10/dist-packages (from torch) (10.9.0.58)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch) (11.10.3.66)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.13.0)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.10/dist-packages (from torch) (10.2.10.91)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.10/dist-packages (from torch) (11.7.91)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.10/dist-packages (from torch) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.0.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: wheel in /usr/lib/python3/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.37.1)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (59.6.0)\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (4.0.0)\n",
      "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (18.1.8)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torchinfo in /home/aerial/.local/lib/python3.10/site-packages (1.8.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikit-learn in /home/aerial/.local/lib/python3.10/site-packages (1.6.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/aerial/.local/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.15.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/aerial/.local/lib/python3.10/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.3)\n"
     ]
    }
   ],
   "source": [
    "import heapq\n",
    "import datetime\n",
    "\n",
    "# Check platform.\n",
    "import platform\n",
    "if platform.machine() not in ['x86_64', 'aarch64']:\n",
    "    raise SystemExit(\"Unsupported platform!\")\n",
    "\n",
    "import math\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Connecting to clickhouse on remote server\n",
    "import clickhouse_connect\n",
    "\n",
    "# Import the channel estimator and some utilities for converting\n",
    "# the DMRS fields in the right format from the SCF FAPI format that the dataset follows.\n",
    "from aerial.phy5g.algorithms import ChannelEstimator\n",
    "from aerial.util.fapi import dmrs_fapi_to_bit_array\n",
    "\n",
    "!pip3 install torch\n",
    "!pip3 install torchinfo\n",
    "# !pip install torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# !pip install torchinfo\n",
    "from torchinfo import summary\n",
    "\n",
    "clickhouse_client = clickhouse_connect.get_client(host='localhost')\n",
    "\n",
    "!pip install scikit-learn\n",
    "from sklearn.metrics import r2_score  # Import RÂ² score function\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8a0a2a5-9dde-4e91-af8d-49521d1e40b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(arr, window_size):\n",
    "    weights = np.ones(window_size) / window_size\n",
    "    return np.convolve(arr, weights, mode='valid')\n",
    "\n",
    "def get_throughput(arr):\n",
    "    mv_avg = moving_average(arr, 20)\n",
    "    largest_numbers = heapq.nlargest(1, mv_avg)\n",
    "    return sum(largest_numbers) / 1\n",
    "    # print(len(arr))\n",
    "    # n = 1\n",
    "    # if len(arr) < n:\n",
    "    #     return sum(arr) / len(arr)  # If there are less than 10 elements, take the average of all\n",
    "    # largest_numbers = heapq.nlargest(n, arr)  # Get the n largest numbers\n",
    "    # return sum(largest_numbers) / n  # Compute the average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50d7a25d-5f66-4e0d-bac3-e84ad4dccb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_IDs = [\n",
    "    # '582c3021631e4b8890518c458a83b560',\n",
    "    # '8501c8ec60bc4766a35c2e30eaf42b28',\n",
    "    '81ce1181052e4a868dfe5a90243b4136',\n",
    "    'c3b0ccf5e133483595578598d741af89',\n",
    "    '7b6be542d6264ede9482eaac2f0891f5',\n",
    "    # '26d5937541f34de4ad13acc1ea42b9d7',\n",
    "    'd9d07b91e64e488c87a04936c478f45d',\n",
    "    '35599c707523406b887334ca6d3801c3',\n",
    "    'c18c4abb9757456b81a79adad377ea5a',\n",
    "    'cb8bbf840bd74f51849d62efcaf6fcdc',\n",
    "    '743fa1e3f6b142fe83a5237528fd5516',\n",
    "    # '8476930f9a894802b477e26e5db5813f',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4480e48c-7ee6-4d1a-9c8f-4df6c6fb32ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " --- Test ID: 81ce1181052e4a868dfe5a90243b4136\n",
      "2025-04-04 17:32:04\n",
      "2025-04-04 17:38:16\n",
      "17.918505502766358\n",
      "Query l2s from 2025-04-04 17:32:04 to 2025-04-04 17:38:16\n",
      "Query fapis from 2025-04-04 17:32:04 to 2025-04-04 17:38:16\n",
      "Query fhs from 2025-04-04 17:32:04 to 2025-04-04 17:32:34\n",
      "Query fhs from 2025-04-04 17:32:34 to 2025-04-04 17:33:04\n"
     ]
    }
   ],
   "source": [
    "lower_edge = 15\n",
    "upper_edge = 1582\n",
    "n_sub = num_subcarriers = 1596\n",
    "keep_indices = np.concatenate((np.arange(0, lower_edge-1), np.arange(upper_edge+1, n_sub)))\n",
    "\n",
    "dataset = []\n",
    "for test_id in TEST_IDs:\n",
    "    print(f\" --- Test ID: {test_id}\")\n",
    "    query = f\"\"\"\n",
    "            SELECT *\n",
    "            FROM iperf3db\n",
    "            WHERE test_id = '{test_id}' AND direction = 0\n",
    "            ORDER BY timestamp\n",
    "            \"\"\"\n",
    "    # print(query)\n",
    "    results =  clickhouse_client.query_df(query)\n",
    "    # print(len(results))\n",
    "    # print(results)\n",
    "\n",
    "    start_timestamp = results['timestamp'][0] \n",
    "    stop_timestamp = results['timestamp'].iloc[-1]\n",
    "    print(start_timestamp)\n",
    "    print(stop_timestamp)\n",
    "    ul_throughputs = results[\"throughput_mbps\"].to_numpy()\n",
    "    max_ul_throughput = get_throughput(ul_throughputs)\n",
    "    print(max_ul_throughput)\n",
    "\n",
    "    query = f\"\"\"\n",
    "    SELECT * FROM MAC_KPIs \n",
    "    WHERE TsTaiNs BETWEEN '{start_timestamp}' AND '{stop_timestamp}'\n",
    "    ORDER BY TsTaiNs DESC\n",
    "    \"\"\"\n",
    "    # print(query)\n",
    "    print(f'Query l2s from {start_timestamp} to {stop_timestamp}')\n",
    "    l2s = clickhouse_client.query_df(query)\n",
    "\n",
    "    query = f\"\"\"\n",
    "    SELECT * FROM fapi \n",
    "    WHERE TsTaiNs BETWEEN '{start_timestamp}' AND '{stop_timestamp}'\n",
    "    ORDER BY TsTaiNs DESC\n",
    "    \"\"\"\n",
    "    print(f'Query fapis from {start_timestamp} to {stop_timestamp}')\n",
    "    fapis = clickhouse_client.query_df(query)\n",
    "\n",
    "    # query = f\"\"\"\n",
    "    # SELECT * FROM fh \n",
    "    # WHERE TsTaiNs BETWEEN '{start_timestamp}' AND '{stop_timestamp}'\n",
    "    # ORDER BY TsTaiNs DESC\n",
    "    # \"\"\"\n",
    "    # fhs = clickhouse_client.query_df(query)\n",
    "\n",
    "    fhs = pd.DataFrame()\n",
    "    interval_seconds = 30\n",
    "    current_start = start_timestamp\n",
    "    while current_start < stop_timestamp:\n",
    "        current_end = min(current_start + datetime.timedelta(seconds=interval_seconds), stop_timestamp)\n",
    "        query = f\"\"\"\n",
    "        SELECT TsTaiNs,fhData FROM fh_tai\n",
    "        WHERE TsTaiNs BETWEEN '{current_start}' AND '{current_end}'\n",
    "        ORDER BY TsTaiNs DESC\n",
    "        \"\"\"\n",
    "        print(f'Query fhs from {current_start} to {current_end}')\n",
    "        df = clickhouse_client.query_df(query)\n",
    "\n",
    "        indices_to_drop = []\n",
    "        for index, fh in df.iterrows():\n",
    "            fh_samp = np.array(fh.fhData, dtype=np.float32)\n",
    "            rx_slot = np.swapaxes(fh_samp.view(np.complex64).reshape(4, 14, 273*12),2,0)\n",
    "\n",
    "            try:\n",
    "                fapi = fapis.loc[fapis['TsTaiNs'] == fh.TsTaiNs].iloc[0].to_dict()\n",
    "            except:\n",
    "                indices_to_drop.append(index)\n",
    "                print(f\"Cannot find fapi at {fh.TsTaiNs}\")\n",
    "                continue\n",
    "            rx_slot[fapi['rbStart'] * 12 : fapi['rbSize'] * 12, fapi['StartSymbolIndex'] : fapi['NrOfSymbols'], :] = 0\n",
    "            rx = rx_slot[0 : 133 * 12, : 6, 0]\n",
    "            \n",
    "            rdft2 = np.fft.fft2(rx)\n",
    "            rtemp = rdft2[keep_indices, :]  \n",
    "            rtrunc = np.fft.ifft2(rtemp)\n",
    "\n",
    "            df.at[index, 'fhData'] = rtrunc\n",
    "        # Drop rows after the loop\n",
    "        df.drop(indices_to_drop, inplace=True)\n",
    "\n",
    "        fhs = pd.concat([fhs, df], ignore_index=True)\n",
    "        current_start += datetime.timedelta(seconds=interval_seconds)\n",
    "        # break\n",
    "\n",
    "    # print(results)\n",
    "    dataset.append({\n",
    "        \"y\" : max_ul_throughput,\n",
    "        \"xs\" : {\n",
    "            \"l2\": l2s,\n",
    "            \"fapi\": fapis,\n",
    "            \"fh\": fhs,\n",
    "        },\n",
    "    })\n",
    "    with open(f'dataset_{test_id}.pkl', 'wb') as file:\n",
    "        pickle.dump(dataset, file)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43aecd4a-3122-4d28-852d-c385f1887ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(dataset))\n",
    "# print(dataset[0]['xs']['fh']['fhData'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57464694-e16e-4e6b-8810-41cdc75a4c83",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'dataset_2.pk'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m      2\u001b[0m     d1 \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(file)\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdataset_2.pk\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m      4\u001b[0m     d2 \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(file)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset3.pk\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dataset_2.pk'"
     ]
    }
   ],
   "source": [
    "with open('dataset.pkl', 'rb') as file:\n",
    "    d1 = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "909c3b6f-4ee9-4dba-a759-1bab3e149eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset_2.pkl', 'rb') as file:\n",
    "    d2 = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a6214236-3ca0-41e3-a21f-51138c0b4102",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset_3.pkl', 'rb') as file:\n",
    "    d3 = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d1f7de8e-ed41-4989-b866-05b59f814aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(len(d3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b435de65-ccbc-43cd-afb8-5f29c1b8ccb7",
   "metadata": {},
   "outputs": [
    {
     "ename": "PicklingError",
     "evalue": "memo id too large for LONG_BINGET",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m combined_dataset \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((d1, d2, d3))\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcombined_dataset.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcombined_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mPicklingError\u001b[0m: memo id too large for LONG_BINGET"
     ]
    }
   ],
   "source": [
    "combined_dataset = np.concatenate((d1, d2, d3))\n",
    "with open('combined_dataset.pkl', 'wb') as file:\n",
    "    pickle.dump(combined_dataset, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66d6261-0fd9-425f-966c-848476462f37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
